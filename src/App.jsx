import React, { useState, useEffect, useRef } from 'react';
import './App.css';

const KoreanLearningApp = () => {
  const [messages, setMessages] = useState([]);
  const [isRecording, setIsRecording] = useState(false);
  const [micPermission, setMicPermission] = useState(null);
  const [settings, setSettings] = useState({
    voiceGender: 'female',
    userLevel: [],
  });
  const [isProcessing, setIsProcessing] = useState(false);
  const [currentAudioPlaying, setCurrentAudioPlaying] = useState(null);
  
  const mediaRecorderRef = useRef(null);
  const audioChunksRef = useRef([]);
  const recognitionRef = useRef(null);

  const callOpenAI = async (endpoint, body, method = 'POST') => {
    const response = await fetch('/api/openai', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        endpoint,
        method,
        body
      })
    });

    if (!response.ok) {
      throw new Error(`API call failed: ${response.statusText}`);
    }

    return response;
  };

  useEffect(() => {
    requestMicrophonePermission();
  }, []);

  const requestMicrophonePermission = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      stream.getTracks().forEach(track => track.stop());
      setMicPermission('granted');
    } catch (error) {
      console.error('Microphone permission denied:', error);
      setMicPermission('denied');
    }
  };

  useEffect(() => {
    if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
      alert('Tr√¨nh duy·ªát kh√¥ng h·ªó tr·ª£ nh·∫≠n di·ªán gi·ªçng n√≥i. Vui l√≤ng d√πng Chrome ho·∫∑c Edge.');
      return;
    }
    
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    recognitionRef.current = new SpeechRecognition();
    recognitionRef.current.lang = 'ko-KR';
    recognitionRef.current.continuous = false;
    recognitionRef.current.interimResults = false;
    
    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.stop();
      }
    };
  }, []);

  const handleMouseDown = async () => {
    if (micPermission !== 'granted') {
      alert('Vui l√≤ng c·∫•p quy·ªÅn microphone tr∆∞·ªõc');
      return;
    }
    
    setIsRecording(true);
    audioChunksRef.current = [];
    
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      mediaRecorderRef.current = new MediaRecorder(stream);
      
      mediaRecorderRef.current.ondataavailable = (event) => {
        audioChunksRef.current.push(event.data);
      };
      
      mediaRecorderRef.current.start();
      
      if (recognitionRef.current) {
        // G√°n event handlers TR∆Ø·ªöC KHI start
        recognitionRef.current.onresult = async (event) => {
          const transcript = event.results[0][0].transcript;
          console.log('Speech recognized:', transcript);
          
          if (transcript && transcript.trim().length > 0) {
            await processUserInput(transcript);
          }
        };
        
        recognitionRef.current.onerror = (event) => {
          console.error('Speech recognition error:', event.error);
          if (event.error !== 'no-speech') {
            alert(`L·ªói nh·∫≠n di·ªán: ${event.error}`);
          }
        };
        
        recognitionRef.current.onend = () => {
          console.log('Speech recognition ended');
        };
        
        recognitionRef.current.start();
        console.log('Speech recognition started');
      }
    } catch (error) {
      console.error('Error starting recording:', error);
      setIsRecording(false);
      alert(`L·ªói kh·ªüi ƒë·ªông: ${error.message}`);
    }
  };

  const handleMouseUp = () => {
    if (!isRecording) return;
    
    console.log('Stopping recording...');
    setIsRecording(false);
    
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop();
      
      mediaRecorderRef.current.onstop = async () => {
        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
        mediaRecorderRef.current.stream.getTracks().forEach(track => track.stop());
        
        if (audioBlob.size < 1000) {
          console.log('No speech detected');
          return;
        }
      };
    }
  };

  const processUserInput = async (userText) => {
    setIsProcessing(true);
    
    try {
      console.log('Processing input:', userText);
      
      const correctionResponse = await callOpenAI('/v1/chat/completions', {
        model: 'gpt-4o-mini',
        messages: [
          {
            role: 'system',
            content: `B·∫°n l√† gi√°o vi√™n ti·∫øng H√†n chuy√™n nghi·ªáp. Nhi·ªám v·ª•:
1. Ki·ªÉm tra c√¢u ti·∫øng H√†n c·ªßa h·ªçc vi√™n v·ªÅ ph√°t √¢m, ng·ªØ ph√°p, t·ª´ v·ª±ng
2. N·∫øu ƒê√öNG HO√ÄN TO√ÄN: tr·∫£ v·ªÅ JSON {"isCorrect": true, "corrected": "", "details": ""}
3. N·∫øu SAI: tr·∫£ v·ªÅ JSON {"isCorrect": false, "corrected": "c√¢u ƒë√£ s·ª≠a", "details": "gi·∫£i th√≠ch l·ªói v√† c√°ch s·ª≠a b·∫±ng ti·∫øng Vi·ªát"}
4. Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng th√™m text kh√°c`
          },
          {
            role: 'user',
            content: `Ki·ªÉm tra c√¢u n√†y: "${userText}"`
          }
        ],
        temperature: 0.3
      });
      
      const correctionData = await correctionResponse.json();
      let correctionResult;
      
      try {
        correctionResult = JSON.parse(correctionData.choices[0].message.content);
      } catch (e) {
        correctionResult = { isCorrect: true, corrected: userText, details: '' };
      }
      
      const userMessage = {
        id: Date.now(),
        type: 'user',
        originalText: userText,
        correctedText: correctionResult.isCorrect ? userText : correctionResult.corrected,
        isCorrect: correctionResult.isCorrect,
        details: correctionResult.details,
        timestamp: new Date()
      };
      
      setMessages(prev => [...prev, userMessage]);
      
      if (!correctionResult.isCorrect) {
        setIsProcessing(false);
        return;
      }
      
      const aiResponse = await callOpenAI('/v1/chat/completions', {
        model: 'gpt-4o-mini',
        messages: [
          {
            role: 'system',
            content: `B·∫°n l√† tr·ª£ l√Ω h·ªçc ti·∫øng H√†n th√¢n thi·ªán.
- Ch·ªâ s·ª≠ d·ª•ng ng·ªØ ph√°p c·∫•p ƒë·ªô: ${settings.userLevel.join(', ') || 's∆° c·∫•p c∆° b·∫£n'}
- LU√îN tr·∫£ l·ªùi b·∫±ng C√ÇU ƒê·∫¶Y ƒê·ª¶ ti·∫øng H√†n
- Tr·∫£ v·ªÅ JSON: {"response": "c√¢u tr·∫£ l·ªùi", "vocabulary": [], "grammar": []}`
          },
          {
            role: 'user',
            content: correctionResult.corrected
          }
        ],
        temperature: 0.7
      });
      
      const aiData = await aiResponse.json();
      let aiResult;
      
      try {
        aiResult = JSON.parse(aiData.choices[0].message.content);
      } catch (e) {
        aiResult = {
          response: aiData.choices[0].message.content,
          vocabulary: [],
          grammar: []
        };
      }
      
      const aiMessage = {
        id: Date.now() + 1,
        type: 'ai',
        text: aiResult.response,
        vocabulary: aiResult.vocabulary,
        grammar: aiResult.grammar,
        timestamp: new Date(),
        audioUrl: null
      };
      
      setMessages(prev => [...prev, aiMessage]);
      await playTTS(aiMessage.id, aiResult.response);
      
    } catch (error) {
      console.error('Error processing:', error);
      alert(`L·ªói x·ª≠ l√Ω: ${error.message}`);
    } finally {
      setIsProcessing(false);
    }
  };

  const playTTS = async (messageId, text) => {
    try {
      setCurrentAudioPlaying(messageId);
      
      const ttsResponse = await callOpenAI('/v1/audio/speech', {
        model: 'tts-1',
        input: text,
        voice: settings.voiceGender === 'female' ? 'nova' : 'onyx',
        speed: 1.0
      });
      
      const audioBlob = await ttsResponse.blob();
      const audioUrl = URL.createObjectURL(audioBlob);
      
      setMessages(prev => prev.map(msg => 
        msg.id === messageId ? { ...msg, audioUrl } : msg
      ));
      
      const audio = new Audio(audioUrl);
      
      audio.onloadedmetadata = () => {
        displayTextWithTyping(messageId, text, audio.duration * 1000);
      };
      
      audio.onended = () => {
        setCurrentAudioPlaying(null);
      };
      
      audio.onerror = (e) => {
        console.error('Audio playback error:', e);
        setCurrentAudioPlaying(null);
      };
      
      await audio.play();
      
    } catch (error) {
      console.error('TTS Error:', error);
      setCurrentAudioPlaying(null);
    }
  };

  const displayTextWithTyping = (messageId, fullText, duration) => {
    const characters = fullText.split('');
    const intervalTime = Math.max(duration / characters.length, 50);
    let currentIndex = 0;
    
    const interval = setInterval(() => {
      if (currentIndex <= characters.length) {
        setMessages(prev => prev.map(msg => 
          msg.id === messageId 
            ? { ...msg, displayedText: characters.slice(0, currentIndex).join('') }
            : msg
        ));
        currentIndex++;
      } else {
        clearInterval(interval);
      }
    }, intervalTime);
  };

  const replayAudio = async (message) => {
    if (message.audioUrl) {
      setCurrentAudioPlaying(message.id);
      const audio = new Audio(message.audioUrl);
      audio.onended = () => setCurrentAudioPlaying(null);
      await audio.play();
    } else {
      await playTTS(message.id, message.text);
    }
  };

  return (
    <div className="korean-app">
      <header className="app-header">
        <div className="logo">
          <span className="korean-flag">üá∞üá∑</span>
          <h1>ÌïúÍµ≠Ïñ¥ ÌïôÏäµ Korean Learning App</h1>
        </div>
        <div className="level-display">
          Î†àÎ≤®: {settings.userLevel.length || 0} Î¨∏Î≤ï
        </div>
      </header>
      
      {micPermission === 'denied' && (
        <div className="permission-alert">
          <div className="alert-content">
            <h2>‚ö†Ô∏è C·∫ßn quy·ªÅn Microphone</h2>
            <p>·ª®ng d·ª•ng c·∫ßn quy·ªÅn truy c·∫≠p microphone ƒë·ªÉ ghi √¢m gi·ªçng n√≥i c·ªßa b·∫°n.</p>
            <button onClick={requestMicrophonePermission} className="btn-primary">
              C·∫•p quy·ªÅn Microphone
            </button>
          </div>
        </div>
      )}
      
      {micPermission === null && (
        <div className="permission-alert">
          <div className="alert-content">
            <h2>ƒêang ki·ªÉm tra quy·ªÅn microphone...</h2>
          </div>
        </div>
      )}
      
      {micPermission === 'granted' && (
        <>
          <div className="chat-container">
            {messages.length === 0 && (
              <div className="welcome-message">
                <h2>ÌôòÏòÅÌï©ÎãàÎã§! Ch√†o m·ª´ng b·∫°n ƒë·∫øn v·ªõi ·ª©ng d·ª•ng h·ªçc ti·∫øng H√†n</h2>
                <p>Nh·∫•n gi·ªØ n√∫t microphone ƒë·ªÉ b·∫Øt ƒë·∫ßu n√≥i ti·∫øng H√†n</p>
              </div>
            )}
            
            {messages.map((message) => (
              <div key={message.id} className={`message ${message.type}`}>
                {message.type === 'user' ? (
                  <div className="user-message">
                    <div className="message-bubble">
                      <div className={`user-text ${!message.isCorrect ? 'incorrect' : ''}`}>
                        {!message.isCorrect && (
                          <span className="original-text" style={{textDecoration: 'line-through', color: '#ff6b6b'}}>
                            {message.originalText}
                          </span>
                        )}
                        <div className="corrected-text">
                          {message.correctedText}
                        </div>
                      </div>
                      
                      {!message.isCorrect && message.details && (
                        <div className="details-section user-details">
                          <h4>üìù Chi ti·∫øt l·ªói:</h4>
                          <p>{message.details}</p>
                        </div>
                      )}
                    </div>
                  </div>
                ) : (
                  <div className="ai-message">
                    <div className="message-bubble">
                      <div className="ai-text">
                        {message.displayedText || message.text}
                        {currentAudioPlaying === message.id && <span className="cursor">|</span>}
                      </div>
                      
                      <button 
                        onClick={() => replayAudio(message)} 
                        className="btn-replay"
                        disabled={currentAudioPlaying === message.id}
                      >
                        {currentAudioPlaying === message.id ? '‚ñ∂Ô∏è ƒêang ph√°t...' : 'üîä Nghe l·∫°i'}
                      </button>
                      
                      <div className="details-section ai-details">
                        <h4>üìö Chi ti·∫øt:</h4>
                        
                        {message.vocabulary && message.vocabulary.length > 0 && (
                          <div className="vocabulary">
                            <h5>T·ª´ v·ª±ng:</h5>
                            <ul>
                              {message.vocabulary.map((item, idx) => (
                                <li key={idx}>{item}</li>
                              ))}
                            </ul>
                          </div>
                        )}
                        
                        {message.grammar && message.grammar.length > 0 && (
                          <div className="grammar">
                            <h5>Ng·ªØ ph√°p:</h5>
                            <ul>
                              {message.grammar.map((item, idx) => (
                                <li key={idx}>{item}</li>
                              ))}
                            </ul>
                          </div>
                        )}
                      </div>
                    </div>
                  </div>
                )}
              </div>
            ))}
            
            {isProcessing && (
              <div className="processing">
                <div className="spinner"></div>
                <p>ƒêang x·ª≠ l√Ω...</p>
              </div>
            )}
          </div>
          
          <div className="control-panel">
            <button
              className={`btn-record ${isRecording ? 'recording' : ''}`}
              onMouseDown={handleMouseDown}
              onMouseUp={handleMouseUp}
              onTouchStart={handleMouseDown}
              onTouchEnd={handleMouseUp}
              disabled={isProcessing}
            >
              {isRecording ? 'üé§ ƒêang ghi...' : 'üé§ Nh·∫•n gi·ªØ ƒë·ªÉ n√≥i'}
            </button>
            
            <div className="settings">
              <label>
                Gi·ªçng AI:
                <select 
                  value={settings.voiceGender} 
                  onChange={(e) => setSettings({...settings, voiceGender: e.target.value})}
                >
                  <option value="female">Ïó¨ÏÑ± (N·ªØ)</option>
                  <option value="male">ÎÇ®ÏÑ± (Nam)</option>
                </select>
              </label>
              
              <button 
                className="btn-settings"
                onClick={() => {
                  const level = prompt('Nh·∫≠p ng·ªØ ph√°p b·∫°n ƒë√£ bi·∫øt (c√°ch nhau b·∫±ng d·∫•u ph·∫©y):\nV√≠ d·ª•: -Ïù¥ÏóêÏöî/ÏòàÏöî, -ÏïÑÏöî/Ïñ¥Ïöî, -„Ñπ Í±∞ÏòàÏöî');
                  if (level) {
                    setSettings({...settings, userLevel: level.split(',').map(s => s.trim())});
                  }
                }}
              >
                ‚öôÔ∏è C√†i ƒë·∫∑t tr√¨nh ƒë·ªô
              </button>
            </div>
          </div>
        </>
      )}
    </div>
  );
};

export default KoreanLearningApp;
