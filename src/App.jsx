import React, { useState, useEffect, useRef } from 'react';
import './App.css';

const KoreanLearningApp = () => {
  const [messages, setMessages] = useState([]);
  const [isRecording, setIsRecording] = useState(false);
  const [micPermission, setMicPermission] = useState(null);
  const [settings, setSettings] = useState({
    voiceGender: 'female',
    userLevel: [],
  });
  const [isProcessing, setIsProcessing] = useState(false);
  const [currentAudioPlaying, setCurrentAudioPlaying] = useState(null);
  const [debugLog, setDebugLog] = useState([]);
  
  const mediaRecorderRef = useRef(null);
  const audioChunksRef = useRef([]);
  const recognitionRef = useRef(null);

  const addDebugLog = (message) => {
    console.log(message);
    setDebugLog(prev => [...prev, `${new Date().toLocaleTimeString()}: ${message}`].slice(-10));
  };

  const callOpenAI = async (endpoint, body, method = 'POST') => {
    const response = await fetch('/api/openai', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        endpoint,
        method,
        body
      })
    });

    if (!response.ok) {
      throw new Error(`API call failed: ${response.statusText}`);
    }

    return response;
  };

  useEffect(() => {
    requestMicrophonePermission();
  }, []);

  const requestMicrophonePermission = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      stream.getTracks().forEach(track => track.stop());
      setMicPermission('granted');
      addDebugLog('âœ… Microphone permission granted');
    } catch (error) {
      console.error('Microphone permission denied:', error);
      setMicPermission('denied');
      addDebugLog('âŒ Microphone permission denied');
    }
  };

  useEffect(() => {
    if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
      alert('TrÃ¬nh duyá»‡t khÃ´ng há»— trá»£ nháº­n diá»‡n giá»ng nÃ³i. Vui lÃ²ng dÃ¹ng Chrome hoáº·c Edge.');
      addDebugLog('âŒ Speech Recognition not supported');
      return;
    }
    
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    recognitionRef.current = new SpeechRecognition();
    recognitionRef.current.lang = 'ko-KR';
    recognitionRef.current.continuous = false;
    recognitionRef.current.interimResults = false;
    recognitionRef.current.maxAlternatives = 1;
    addDebugLog('âœ… Speech Recognition initialized');
    
    return () => {
      if (recognitionRef.current) {
        try {
          recognitionRef.current.stop();
        } catch (e) {
          // Ignore errors on cleanup
        }
      }
    };
  }, []);

  const handleMouseDown = async () => {
    if (micPermission !== 'granted') {
      alert('Vui lÃ²ng cáº¥p quyá»n microphone trÆ°á»›c');
      return;
    }
    
    addDebugLog('â–¶ï¸ Starting recording...');
    setIsRecording(true);
    audioChunksRef.current = [];
    
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        } 
      });
      
      mediaRecorderRef.current = new MediaRecorder(stream);
      
      mediaRecorderRef.current.ondataavailable = (event) => {
        audioChunksRef.current.push(event.data);
      };
      
      mediaRecorderRef.current.start();
      addDebugLog('ğŸ™ï¸ MediaRecorder started');
      
      if (recognitionRef.current) {
        // Set up event handlers
        recognitionRef.current.onresult = async (event) => {
          const transcript = event.results[0][0].transcript;
          const confidence = event.results[0][0].confidence;
          addDebugLog(`ğŸ¯ Recognized: "${transcript}" (${(confidence * 100).toFixed(0)}%)`);
          
          if (transcript && transcript.trim().length > 0) {
            await processUserInput(transcript);
          }
        };
        
        recognitionRef.current.onerror = (event) => {
          addDebugLog(`âŒ Speech error: ${event.error}`);
          if (event.error === 'no-speech') {
            alert('KhÃ´ng nghe tháº¥y giá»ng nÃ³i. HÃ£y nÃ³i TO vÃ  RÃ• hÆ¡n!');
          } else if (event.error === 'audio-capture') {
            alert('Lá»—i microphone. Kiá»ƒm tra láº¡i quyá»n truy cáº­p!');
          } else if (event.error !== 'aborted') {
            alert(`Lá»—i nháº­n diá»‡n: ${event.error}`);
          }
        };
        
        recognitionRef.current.onend = () => {
          addDebugLog('â¹ï¸ Speech recognition ended');
        };
        
        recognitionRef.current.onstart = () => {
          addDebugLog('ğŸ¤ Speech recognition started - HÃƒY NÃ“I NGAY!');
        };
        
        recognitionRef.current.onspeechstart = () => {
          addDebugLog('ğŸ—£ï¸ Speech detected!');
        };
        
        recognitionRef.current.onspeechend = () => {
          addDebugLog('ğŸ”‡ Speech ended, processing...');
        };
        
        recognitionRef.current.onaudiostart = () => {
          addDebugLog('ğŸ”Š Audio input started');
        };
        
        recognitionRef.current.onaudioend = () => {
          addDebugLog('ğŸ”‡ Audio input ended');
        };
        
        try {
          recognitionRef.current.start();
          addDebugLog('âœ… Recognition started - NÃ“I TIáº¾NG HÃ€N NGAY!');
        } catch (e) {
          addDebugLog(`âŒ Start error: ${e.message}`);
        }
      }
    } catch (error) {
      addDebugLog(`âŒ Error: ${error.message}`);
      setIsRecording(false);
      alert(`Lá»—i khá»Ÿi Ä‘á»™ng: ${error.message}`);
    }
  };

  const handleMouseUp = () => {
    if (!isRecording) return;
    
    addDebugLog('â¸ï¸ Stopping recording...');
    setIsRecording(false);
    
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop();
      
      mediaRecorderRef.current.onstop = async () => {
        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
        mediaRecorderRef.current.stream.getTracks().forEach(track => track.stop());
        addDebugLog(`ğŸ“¦ Audio blob size: ${audioBlob.size} bytes`);
      };
    }
  };

  const processUserInput = async (userText) => {
    addDebugLog(`ğŸ”„ Processing: "${userText}"`);
    setIsProcessing(true);
    
    try {
      const correctionResponse = await callOpenAI('/v1/chat/completions', {
        model: 'gpt-4o-mini',
        messages: [
          {
            role: 'system',
            content: `You are a Korean language teacher. Task:
1. Check if the Korean sentence is grammatically correct
2. If CORRECT: return JSON {"isCorrect": true, "corrected": "", "details": ""}
3. If INCORRECT: return JSON {"isCorrect": false, "corrected": "corrected sentence", "details": "explanation in Vietnamese"}
4. Return ONLY JSON, no other text`
          },
          {
            role: 'user',
            content: `Check this Korean sentence: "${userText}"`
          }
        ],
        temperature: 0.3
      });
      
      addDebugLog('âœ… Got correction response');
      const correctionData = await correctionResponse.json();
      let correctionResult;
      
      try {
        const content = correctionData.choices[0].message.content;
        const cleaned = content.replace(/``````\n?/g, '').trim();
        correctionResult = JSON.parse(cleaned);
      } catch (e) {
        correctionResult = { isCorrect: true, corrected: userText, details: '' };
      }
      
      const userMessage = {
        id: Date.now(),
        type: 'user',
        originalText: userText,
        correctedText: correctionResult.isCorrect ? userText : correctionResult.corrected,
        isCorrect: correctionResult.isCorrect,
        details: correctionResult.details,
        timestamp: new Date()
      };
      
      setMessages(prev => [...prev, userMessage]);
      addDebugLog(`ğŸ“ User message added (correct: ${correctionResult.isCorrect})`);
      
      if (!correctionResult.isCorrect) {
        setIsProcessing(false);
        return;
      }
      
      const aiResponse = await callOpenAI('/v1/chat/completions', {
        model: 'gpt-4o-mini',
        messages: [
          {
            role: 'system',
            content: `You are a Korean language learning assistant. CRITICAL RULES:

1. You MUST respond ONLY in KOREAN (í•œêµ­ì–´). NO Vietnamese. NO English.
2. Use grammar level: ${settings.userLevel.join(', ') || 'beginner (ì´ˆê¸‰)'}
3. Your response must be a natural, conversational Korean sentence
4. Return ONLY this JSON format (no markdown, no code blocks):

{"response": "í•œêµ­ì–´ë¡œë§Œ ì‘ë‹µ", "vocabulary": ["ë‹¨ì–´: Vietnamese meaning"], "grammar": ["ë¬¸ë²•: Vietnamese explanation"]}

Example:
User: "ì•ˆë…•í•˜ì„¸ìš”"
You return:
{"response": "ì•ˆë…•í•˜ì„¸ìš”! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”. ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë•Œìš”?", "vocabulary": ["ë§Œë‚˜ë‹¤: gáº·p", "ë°˜ê°‘ë‹¤: vui má»«ng", "ê¸°ë¶„: tÃ¢m tráº¡ng"], "grammar": ["-ì•„/ì–´ìš”: thá»ƒ lá»‹ch sá»± thÃ¢n máº­t"]}

Remember: Response MUST be 100% Korean language only!`
          },
          {
            role: 'user',
            content: correctionResult.corrected
          }
        ],
        temperature: 0.7
      });
      
      addDebugLog('âœ… Got AI response');
      const aiData = await aiResponse.json();
      let aiResult;
      
      try {
        const responseText = aiData.choices[0].message.content;
        const cleanedText = responseText.replace(/``````\n?/g, '').trim();
        aiResult = JSON.parse(cleanedText);
      } catch (e) {
        addDebugLog(`âš ï¸ JSON parse error, using fallback`);
        aiResult = {
          response: aiData.choices[0].message.content,
          vocabulary: [],
          grammar: []
        };
      }
      
      const aiMessage = {
        id: Date.now() + 1,
        type: 'ai',
        text: aiResult.response,
        vocabulary: aiResult.vocabulary,
        grammar: aiResult.grammar,
        timestamp: new Date(),
        audioUrl: null
      };
      
      setMessages(prev => [...prev, aiMessage]);
      addDebugLog('ğŸ¤– AI message added');
      await playTTS(aiMessage.id, aiResult.response);
      
    } catch (error) {
      addDebugLog(`âŒ Processing error: ${error.message}`);
      alert(`Lá»—i xá»­ lÃ½: ${error.message}`);
    } finally {
      setIsProcessing(false);
    }
  };

  const playTTS = async (messageId, text) => {
    try {
      addDebugLog('ğŸ”Š Playing TTS...');
      setCurrentAudioPlaying(messageId);
      
      const ttsResponse = await callOpenAI('/v1/audio/speech', {
        model: 'tts-1',
        input: text,
        voice: settings.voiceGender === 'female' ? 'nova' : 'onyx',
        speed: 1.0
      });
      
      const audioBlob = await ttsResponse.blob();
      const audioUrl = URL.createObjectURL(audioBlob);
      
      setMessages(prev => prev.map(msg => 
        msg.id === messageId ? { ...msg, audioUrl } : msg
      ));
      
      const audio = new Audio(audioUrl);
      
      audio.onloadedmetadata = () => {
        displayTextWithTyping(messageId, text, audio.duration * 1000);
      };
      
      audio.onended = () => {
        setCurrentAudioPlaying(null);
        addDebugLog('âœ… TTS playback ended');
      };
      
      audio.onerror = (e) => {
        addDebugLog(`âŒ Audio error: ${e}`);
        setCurrentAudioPlaying(null);
      };
      
      await audio.play();
      addDebugLog('â–¶ï¸ TTS started');
      
    } catch (error) {
      addDebugLog(`âŒ TTS error: ${error.message}`);
      setCurrentAudioPlaying(null);
    }
  };

  const displayTextWithTyping = (messageId, fullText, duration) => {
    const characters = fullText.split('');
    const intervalTime = Math.max(duration / characters.length, 50);
    let currentIndex = 0;
    
    const interval = setInterval(() => {
      if (currentIndex <= characters.length) {
        setMessages(prev => prev.map(msg => 
          msg.id === messageId 
            ? { ...msg, displayedText: characters.slice(0, currentIndex).join('') }
            : msg
        ));
        currentIndex++;
      } else {
        clearInterval(interval);
      }
    }, intervalTime);
  };

  const replayAudio = async (message) => {
    if (message.audioUrl) {
      setCurrentAudioPlaying(message.id);
      const audio = new Audio(message.audioUrl);
      audio.onended = () => setCurrentAudioPlaying(null);
      await audio.play();
    } else {
      await playTTS(message.id, message.text);
    }
  };

  const testWithText = () => {
    const testText = prompt('Nháº­p cÃ¢u tiáº¿ng HÃ n Ä‘á»ƒ test:\n(VD: ì•ˆë…•í•˜ì„¸ìš”)');
    if (testText) {
      processUserInput(testText);
    }
  };

  return (
    <div className="korean-app">
      <header className="app-header">
        <div className="logo">
          <span className="korean-flag">ğŸ‡°ğŸ‡·</span>
          <h1>í•œêµ­ì–´ í•™ìŠµ Korean Learning App</h1>
        </div>
        <div className="level-display">
          ë ˆë²¨: {settings.userLevel.length || 0} ë¬¸ë²•
        </div>
      </header>
      
      {micPermission === 'denied' && (
        <div className="permission-alert">
          <div className="alert-content">
            <h2>âš ï¸ Cáº§n quyá»n Microphone</h2>
            <p>á»¨ng dá»¥ng cáº§n quyá»n truy cáº­p microphone Ä‘á»ƒ ghi Ã¢m giá»ng nÃ³i cá»§a báº¡n.</p>
            <button onClick={requestMicrophonePermission} className="btn-primary">
              Cáº¥p quyá»n Microphone
            </button>
          </div>
        </div>
      )}
      
      {micPermission === null && (
        <div className="permission-alert">
          <div className="alert-content">
            <h2>Äang kiá»ƒm tra quyá»n microphone...</h2>
          </div>
        </div>
      )}
      
      {micPermission === 'granted' && (
        <>
          <div style={{background: '#f0f0f0', padding: '10px', fontSize: '10px', maxHeight: '100px', overflow: 'auto', margin: '10px'}}>
            <strong>Debug Log:</strong>
            {debugLog.map((log, idx) => <div key={idx}>{log}</div>)}
          </div>

          <div className="chat-container">
            {messages.length === 0 && (
              <div className="welcome-message">
                <h2>í™˜ì˜í•©ë‹ˆë‹¤! ChÃ o má»«ng báº¡n Ä‘áº¿n vá»›i á»©ng dá»¥ng há»c tiáº¿ng HÃ n</h2>
                <p><strong>ğŸ¤ Nháº¥n giá»¯ nÃºt â†’ NÃ³i TO vÃ  RÃ• báº±ng tiáº¿ng HÃ n â†’ Tháº£ nÃºt</strong></p>
                <p style={{fontSize: '14px', color: '#666'}}>LÆ°u Ã½: NÃ³i trong mÃ´i trÆ°á»ng yÃªn tÄ©nh, phÃ¡t Ã¢m rÃµ rÃ ng</p>
              </div>
            )}
            
            {messages.map((message) => (
              <div key={message.id} className={`message ${message.type}`}>
                {message.type === 'user' ? (
                  <div className="user-message">
                    <div className="message-bubble">
                      <div className={`user-text ${!message.isCorrect ? 'incorrect' : ''}`}>
                        {!message.isCorrect && (
                          <span className="original-text" style={{textDecoration: 'line-through', color: '#ff6b6b'}}>
                            {message.originalText}
                          </span>
                        )}
                        <div className="corrected-text">
                          {message.correctedText}
                        </div>
                      </div>
                      
                      {!message.isCorrect && message.details && (
                        <div className="details-section user-details">
                          <h4>ğŸ“ Chi tiáº¿t lá»—i:</h4>
                          <p>{message.details}</p>
                        </div>
                      )}
                    </div>
                  </div>
                ) : (
                  <div className="ai-message">
                    <div className="message-bubble">
                      <div className="ai-text">
                        {message.displayedText || message.text}
                        {currentAudioPlaying === message.id && <span className="cursor">|</span>}
                      </div>
                      
                      <button 
                        onClick={() => replayAudio(message)} 
                        className="btn-replay"
                        disabled={currentAudioPlaying === message.id}
                      >
                        {currentAudioPlaying === message.id ? 'â–¶ï¸ Äang phÃ¡t...' : 'ğŸ”Š Nghe láº¡i'}
                      </button>
                      
                      <div className="details-section ai-details">
                        <h4>ğŸ“š Chi tiáº¿t:</h4>
                        
                        {message.vocabulary && message.vocabulary.length > 0 && (
                          <div className="vocabulary">
                            <h5>Tá»« vá»±ng:</h5>
                            <ul>
                              {message.vocabulary.map((item, idx) => (
                                <li key={idx}>{item}</li>
                              ))}
                            </ul>
                          </div>
                        )}
                        
                        {message.grammar && message.grammar.length > 0 && (
                          <div className="grammar">
                            <h5>Ngá»¯ phÃ¡p:</h5>
                            <ul>
                              {message.grammar.map((item, idx) => (
                                <li key={idx}>{item}</li>
                              ))}
                            </ul>
                          </div>
                        )}
                      </div>
                    </div>
                  </div>
                )}
              </div>
            ))}
            
            {isProcessing && (
              <div className="processing">
                <div className="spinner"></div>
                <p>Äang xá»­ lÃ½...</p>
              </div>
            )}
          </div>
          
          <div className="control-panel">
            <button
              onClick={testWithText}
              style={{
                background: '#2196F3',
                color: 'white',
                padding: '12px 20px',
                border: 'none',
                borderRadius: '50px',
                fontSize: '14px',
                fontWeight: 'bold',
                cursor: 'pointer',
                marginBottom: '10px',
                width: '100%'
              }}
            >
              ğŸ§ª TEST: Nháº­p text (náº¿u mic khÃ´ng hoáº¡t Ä‘á»™ng)
            </button>

            <button
              className={`btn-record ${isRecording ? 'recording' : ''}`}
              onMouseDown={handleMouseDown}
              onMouseUp={handleMouseUp}
              onTouchStart={handleMouseDown}
              onTouchEnd={handleMouseUp}
              disabled={isProcessing}
            >
              {isRecording ? 'ğŸ¤ ÄANG GHI - HÃƒY NÃ“I!' : 'ğŸ¤ Nháº¥n giá»¯ Ä‘á»ƒ nÃ³i'}
            </button>
            
            <div className="settings">
              <label>
                Giá»ng AI:
                <select 
                  value={settings.voiceGender} 
                  onChange={(e) => setSettings({...settings, voiceGender: e.target.value})}
                >
                  <option value="female">ì—¬ì„± (Ná»¯)</option>
                  <option value="male">ë‚¨ì„± (Nam)</option>
                </select>
              </label>
              
              <button 
                className="btn-settings"
                onClick={() => {
                  const level = prompt('Nháº­p ngá»¯ phÃ¡p báº¡n Ä‘Ã£ biáº¿t (cÃ¡ch nhau báº±ng dáº¥u pháº©y):\nVÃ­ dá»¥: -ì´ì—ìš”/ì˜ˆìš”, -ì•„ìš”/ì–´ìš”, -ã„¹ ê±°ì˜ˆìš”');
                  if (level) {
                    setSettings({...settings, userLevel: level.split(',').map(s => s.trim())});
                  }
                }}
              >
                âš™ï¸ CÃ i Ä‘áº·t trÃ¬nh Ä‘á»™
              </button>
            </div>
          </div>
        </>
      )}
    </div>
  );
};

export default KoreanLearningApp;
